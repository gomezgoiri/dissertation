\section{Experimental Environment}
\label{environment}
We use simulation to study the performance of our solution and compare it against a flooding-based one.
Using a simulation, we can evaluate multiple scenarios and repeat experiments for different approaches under the same conditions.
The source code for the evaluation has been made publicly available\footnote{\url{http://gomezgoiri.net/files/code/gomezgoiri201Xenergy.html}}.

\subsection{Methodology}
Table~\ref{tab:configurationParameters} shows the main parameters of our simulator.
We vary these parameters to simulate a wide range of scenarios.
% Dado que los revisores parece que no hicieron cado de este aviso, mejor quitarlo.
% We omit topology considerations as we assume devices have IP connectivity.


\begin{table}
  \centering
    \begin{tabular}{l p{8cm}}
      \hline
      Name & Description \\
      \hline
      Network size & The number of nodes in a network. In the simulations conducted, all the nodes are \providers{}. \\
      Number of writes & Amount of writes performed during the simulation period. \\
      Number of queries & Amount of queries performed during the simulation period. \\
      Number of \consumers{} & Amount of nodes querying to other nodes in the \Space{}. \\
      Distribution strategy & Our solution or \ac{nb}. \\
      & In \ac{nb} nodes write locally and spread the queries to the rest of the nodes in the \Space{}. \\
      Drop interval & At the beginning of this interval a node abruptly leaves the network. \\
      & At the end, the node joins the network and a new one is chosen to leave it. \\
      % FUTURE WORK: añadir métrica de accuracy [ http://en.wikipedia.org/wiki/Recall_(information_retrieval) ]
      \hline
    \end{tabular}
    \caption {Configuration parameters.}
  \label{tab:configurationParameters}
\end{table}

% Since we wanted to simulate the Triple Space Computing paradigm over HTTP, the communication between the nodes was point to point and the data exchanged were RDF Triples.
% The node discovery process was ignored since it will show similar additional overhead for each strategy.
% It can be considered transversal to what was being measured.
% IG: version del parrafo anterior
% AG: Ok, pero esta es una de las cosas que en el uptade debería volar o cambiar. Sobre todo, porque no hablamos de TSC
%     en ningún otro lado. Puede ser más fácil aún, ya que como asumimos que queremos semántica en WoT, el HTTP está
%     justificado per se.
As we simulate \ac{http}, we assume point to point communication between devices which exchange \ac{rdf} Triples.
We discuss how the discovery process affects the solution in Section~\ref{sec:dynamic}.
In the remaining sections, the node discovery process is omitted as it represents the same overhead for all strategies.

%To represent the data managed by each node, we first considered using LUBM\footnote{\url{http://swat.cse.lehigh.edu/projects/lubm/}}, a synthetic benchmark.
%Unfortunately, it creates instances from very few classes for each node, which makes all the nodes to have the same TBox.
%In our opinion, this does not faithfully represent Internet of Things scenarios with heterogeneous devices sharing disparate information.

%In our second attempt, we found a dataset which could represent this heterogeneity.
To represent the data managed by each node, we use data from diverse sensor network environments.
These data follow the \textit{Semantic Sensor Network Ontology} (SSN) \citeweb{ssn}.
% created by the W3C Semantic Sensor Network Incubator Group. to represent diverse sensor network environments.
SSN has been used in many projects and scenarios to describe semantically the data provided by heterogeneous sensors.
Specifically, we use data from the following datasets:
AEMET metereological dataset \citeweb{aemet},
University of Luebeck Wisebed Sensor Readings \citeweb{luebeck},
\emph{Kno.e.sis} Linked Sensor Data \citeweb{knoesis}
and Bizkaisense \citeweb{bizkaisense}.
These datasets contain descriptions about the sensing stations and the data sensed by them during certain periods.
The analogy between stations which have different sensors and the \ac{iot} devices is reasonable.
The datasets are adapted to provide just one measure of each sensor at each moment (to emulate the storage restrictions from embedded devices) and to use as many stations as nodes has the network (depending on the network size).

However, not only sensors but also personal devices (e.g. mobile phones) usually populate \ac{ubicomp} environments.
To represent such circumstance, we add semantic data of people to represent their profiles \citeweb{morelab_people}.

% poner disponible el código
We use SimPy \citeweb{simpy} to simulate each scenario.
SimPy is a process-based discrete-event simulation language for Python.
To accurately simulate the time needed by each node to provide a response, we consider measures taken from real embedded web servers \citep{gomez-goiri_restful_2012}.
These servers run on a \textit{ConnectPort X2} IP gateway \citeweb{connectportx2} for \textit{Digi}'s \textit{XBee sensors} \citeweb{digixbeesensors} (\textit{XBee} from now on),
on a FoxG20 \citeweb{foxg20} and on a Samsung Galaxy Tab \citeweb{samsunggalaxytab}.
Besides, we also provide measures taken from a regular computer.
Table~\ref{tab:technicalDetails} shows the technical specifications of these devices.


% http://www.gsmarena.com/samsung_p1000_galaxy_tab-3370.php
\begin{table}
  \caption{Technical characteristics of the assessed devices.}
  \small
  \begin{center}
    \begin{tabular}{llrl}
      \hline
      Device & Processor & RAM & Reference\\
      \hline
      XBee &  - & 8 MB  & \citeweb{connectportx2} \\
      FoxG20 & 400Mhz Atmel ARM9 &  64 MB & \citeweb{foxg20} \\
      Samsung Galaxy Tab & 1 GHz Cortex-A8  &  512 MB & \citeweb{samsunggalaxytab} \\
      Regular computer & 2.26 GHz Intel Core 2 Duo  &  4 GB & - \\
      \hline
    \end{tabular}
  \end{center}
  \label{tab:technicalDetails}
\end{table}

These devices serve semantic content through \acs{http}.
Table~\ref{tab:deviceslibraries} shows the platforms and libraries used in each device to implement such servers.

\begin{table}
  \caption{Core libraries used in the semantic \acs{http} server implementations.}
  \small
  \label{tab:deviceslibraries}
  \begin{center}
    \begin{tabular}{llll}
      \hline
      \multirow{2}{*}{Device} & Platform & \multirow{2}{*}{REST libraries} & Semantic \\
      ~ & version & ~ & libraries \\
      \hline
      XBee & Python 2.4 \citeweb{python} & Python Std Lib & None \\
      FoxG20 & Python 2.5 &  Python Std Lib & RDFLib \citeweb{rdflib} \\
      Samsung Galaxy Tab & Android 2.2 \citeweb{android} & Restlet \citeweb{restlet} & Sesame \citeweb{sesame} \\
      % Era técnicamente posible tener Rdf2Go en Android, pero no lo hicimos así.
      Regular computer & Java SE 6.0 \citeweb{javasesix} & Restlet & Rdf2Go (Sesame) \citeweb{rdftogo} \\
      \hline
    \end{tabular}
  \end{center}
\end{table}


Finally, Table~\ref{tab:measures_embedded} details the time needed by each device to answer a request depending on the number of concurrent requests.
As mentioned, the next section's simulations are parametrized according to these measures.

\begin{table}
  \caption{Mean of the measurements taken in different devices with the standard deviation in parenthesis (milliseconds).}
  \begin{center}
	\begin{tabular}{p{2.5cm} r r r r}
	  \hline
	  & \multicolumn{4}{c}{Devices} \\
	  \cline{2-5}
	  Concurrent & \multirow{2}{*}{XBee} & \multirow{2}{*}{FoxG20} & Samsung & Regular  \\
	  requests   &  ~    &   ~     & Galaxy tab & computer \\
	  \hline
	  1  &  77~(1) & 17~~~(0) & 223~(349) & 5~~~(1) \\
	  5  & 392~(8) &  97~(16) & 256~~~(76) & 8~~~(4) \\
	  10 & 775~(8) & 174~(28) & 372~(171) & 13~~~(8) \\
	  15 &  -      & 282~(43) & 497~(191) & 18~(13) \\
% Tampoco creo que haga falta tanto detalle, ¿cuando se van a dar tantas peticiones concurrentes?
%	  20 &  -	    & 375 (30) &  23 (13) &  661 (444) \\
%	  25 &  -	    & 460 (30) &  30 (18) &  748 (288) \\
%	  30 &  -	    & 540 (35) &  38 (22) &  929 (805) \\
%	  35 &  -	    & 632 (29) &  38 (20) & 1029 (672) \\
	  \hline
	\end{tabular}
  \end{center}
  \label{tab:measures_embedded}
\end{table}


\subsection{Performance Metrics}
To evaluate the fundamental properties of the strategies, we use the following metrics:
\begin{itemize}
  \item \textit{Precision}: the fraction of nodes which answered relevant results (those responses which were not \textit{not found} responses).
                            It measures the exactness of the results.
  \item \textit{Recall}: the fraction of relevant answers that are returned.
                         It measures the completeness of the results.% overload, failed search, etc.
  \item \textit{Size}: the size of each type of clue.
%  \item \textit{Throughput}: the average rate of successful message delivery over a communication channel.
%  \item \textit{Idle time}: the average time each node is in an idle state and therefore is consuming less energy.
%  \item \textit{Failed requests}: the amount of request which could not be answered due to the physical limitations of
% the nodes (i.e. both due to timeouts or connection rejections when the server is overloaded).
  \item \textit{Total requests}: the number of \acs{http} requests performed during a simulation.
  \item \textit{Response time}: the average time needed to obtain an \acs{http} response.
  \item \textit{Active time}: the total time spent by each node either querying other nodes or handling a query.
\end{itemize}